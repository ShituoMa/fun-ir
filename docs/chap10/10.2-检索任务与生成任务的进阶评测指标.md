# 检索任务与生成任务的进阶评测指标

## 检索的进阶评价指标

### 第一节：Bpref指标

#### Bpref指标的定义
Bpref（Binary Preference）指标是一种用于评估信息检索系统性能的指标，特别是在用户对文档的相关性有明确偏好时。它通过考虑用户对文档的偏好程度来衡量检索结果的相关性。

#### Bpref指标的计算方法
- **基本思想**：Bpref指标假设用户对每个文档都有一个偏好值，通常为0或1，其中1表示用户偏好该文档，0表示不偏好。
- **计算公式**：
  $$
  \text{Bpref} = \frac{1}{|R|} \sum_{d \in R} \text{pref}(d)
  $$
  其中：
  - $|R|$ 是用户偏好的文档集合的大小。
  - $\text{pref}(d)$ 是文档 $d$ 的偏好值，通常为0或1。

#### Bpref指标的应用场景
- **个性化推荐系统**：在个性化推荐系统中，Bpref可以用于评估系统推荐的文档是否符合用户的个人偏好.
- **用户反馈驱动的检索**：当用户可以对检索结果进行反馈时，Bpref可以帮助评估用户反馈的有效性和检索系统的改进效果.

### 第二节：GMAP指标

#### GMAP指标的定义
GMAP（Group Mean Average Precision）指标是一种用于评估信息检索系统在多个用户群体中的平均性能的指标。它通过计算多个用户群体的平均准确率来衡量系统的整体性能。

#### GMAP指标的计算方法
- **基本思想**：GMAP指标假设每个用户群体都有一个独立的查询集合和相关性判定结果，通过计算每个用户群体的平均准确率，然后取这些平均准确率的平均值。
- **计算公式**：
  $$
  \text{GMAP} = \frac{1}{|G|} \sum_{g \in G} \text{MAP}_g
  $$
  其中：
  - $|G|$ 是用户群体的数量。
  - $\text{MAP}_g$ 是用户群体 $g$ 的平均准确率（MAP）。

#### GMAP指标的应用场景
- **多用户检索系统**：在面向多个用户群体的检索系统中，GMAP可以帮助评估系统在不同用户群体中的性能平衡性.
- **跨领域检索评估**：当评估跨多个领域的检索系统时，GMAP可以用于比较系统在不同领域中的表现.

### 第三节：NDCG指标

#### NDCG指标的定义
NDCG（Normalized Discounted Cumulative Gain）指标是一种用于评估信息检索系统在考虑文档相关性等级的情况下，对检索结果排序质量的指标。它通过计算文档的相关性收益并进行归一化处理来衡量系统的性能。

#### NDCG指标的计算方法
- **基本思想**：NDCG指标假设每个文档都有一个相关性等级，通常为0到5之间的整数，其中5表示最相关。通过计算文档的相关性收益并考虑其在检索结果中的位置，然后进行归一化处理。
- **计算公式**：
  $$
  \text{NDCG}_k = \frac{\text{DCG}_k}{\text{IDCG}_k}
  $$
  其中：
  - $\text{DCG}_k$ 是在前 $k$ 个文档中的折扣累积收益，计算公式为：
    $$
    \text{DCG}_k = \sum_{i=1}^{k} \frac{2^{rel_i} - 1}{\log_2(i+1)}
    $$
    - $rel_i$ 是第 $i$ 个文档的相关性等级.
  - $\text{IDCG}_k$ 是理想情况下的折扣累积收益，即假设相关性最高的文档排在最前面时的DCG值.

#### NDCG指标的应用场景
- **搜索引擎优化**：在搜索引擎中，NDCG可以帮助评估不同排序算法的效果，优化检索结果的排序质量.
- **推荐系统评估**：在推荐系统中，NDCG可以用于评估推荐结果的相关性和排序质量，提高用户满意度.

## 生成的进阶评价指标

### 1. BLEU值
**定义**：BLEU值（Bilingual Evaluation Understudy）是用于评估机器翻译任务准确率的指标。它通过比较模型生成的句子（candidate）与给定的标准译文（reference）之间的n-gram重叠来计算。

**计算公式**：
$$
\text{BLEU} = \exp\left(\sum_{n=1}^{N} \frac{1}{n} \log p_n\right) \cdot BP
$$
其中，\( p_n \) 是n-gram的精确度，BP是长度惩罚因子。

**示例**：
- **BLEU-2**：假设候选句子为 "the cat sat on the mat"，参考句子为 "the cat is on the mat"。候选句子中共有5个bigram：{the cat, cat sat, sat on, on the, the mat}，其中出现在参考中的bigram有 {the cat, on the, the mat}。因此，候选句子的BLEU-2值为 0.6。

**长度惩罚因子**：BLEU值的一个缺陷是，当机器翻译的长度较短时，BLEU得分可能会偏高。为了纠正这一问题，BLEU引入了长度惩罚因子（Brevity Penalty, BP），计算公式为：
$$
BP = \begin{cases} 
1 & \text{if } c > r \\
e^{-(1 - \frac{c}{r})} & \text{if } c \leq r 
\end{cases}
$$
其中，\( c \) 是模型生成文本的长度，\( r \) 是参考文本的长度。

### 2. ROUGE
**定义**：ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是用于评估自动文摘与机器翻译的指标。它通过比较模型生成的文本与一组参考文本之间的重叠单元（如n-gram、词序列等）来计算。

**主要指标**：
- **Recall**：衡量模型生成的文本中与参考文本重叠的单元占参考文本中单元的比例。
- **Precision**：衡量模型生成的文本中与参考文本重叠的单元占模型生成文本中单元的比例。

**计算公式**：
$$
\text{ROUGE-N} = \frac{\sum_{i=1}^{N} \text{count}_{\text{match}}(n\text{-gram}_i)}{\sum_{i=1}^{N} \text{count}(n\text{-gram}_i)}
$$

**示例**：
- **ROUGE-2**：假设系统生成的摘要为 "the cat was found under the bed"，参考摘要为 "the cat was under the bed"。通过比较2-gram，可以计算出ROUGE-2的值。

### 3. METEOR
**定义**：METEOR是综合考虑了Recall与Precision的评价指标，旨在克服BLEU指标的缺陷。

**计算步骤**：
1. **计算unigram的召回率与精确率**：
   - **Precision**：模型生成文本与参考文本中都出现的unigram的数目 / 模型生成文本unigram数目总和。
   - **Recall**：模型生成文本与参考文本中都出现的unigram的数目 / 参考文本unigram数目总和。
2. **计算惩罚因子**：通过将生成文本与参考文本进行对齐，找出不匹配的区域，将生成文本切分成多个chunks，计算chunks的数目。
3. **最终METEOR值**：
   $$
   \text{METEOR} = \frac{10 \cdot \text{F-mean} \cdot \text{Penalty}}{9 \cdot \text{F-mean} + \text{Penalty}}
   $$
   其中，F-mean是召回率与精确率的加权调和平均数，Penalty是惩罚因子。

### 4. Diversity
**定义**：Diversity指标用于评价生成文本词的丰富度，通过计算生成文本中n-gram种类数与总的n-gram数的比值来衡量。

**计算公式**：
$$
\text{Diversity} = \frac{\text{Unique n-grams}}{\text{Total n-grams}}
$$

**示例**：
- 假设生成文本为 "the cat was found under the bed"，其中共有7个unigram，6个bigram，其中有不重复的unigram 6个，不重复的bigram 6个。因此，该句子的unigram的Diversity指标为 0.857（6/7），bigram的Diversity指标为 1。

### 5. 人工评价
**定义**：人工评价通过专业标注员从多个角度对生成文本的质量进行打分，常见的评价维度包括：
- **相关性（Relevance）**：生成的文本是否与给定的标题或主题相关。
- **流利度（Fluency）**：生成的文本是否流利易懂。
- **信息量（Informativeness）**：生成的文本是否包含丰富且有趣的信息。

**评分标准**：通常采用1到5分的评分标准，1分表示最差，5分表示最好。通过多个标注员的评分，可以综合评估生成文本的质量。

