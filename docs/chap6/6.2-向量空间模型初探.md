# 向量空间模型初探

## TF-IDF分数

还记得我们前面在布尔索引那个地方构建的二值化矩阵吗？现在让我们对它做一点修改，矩阵的数值不再是布尔值（出现该单词/未出现该单词），而是把具体的词频记录进去。

![1735639332484](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735639332484.png)

这里，我们引出第一个模型——**BoW模型**，也叫词袋模型。

词袋模型不考虑词在文本中出现的顺序，也就是说“小明比小刚聪明”和“小刚比小明聪明”在词袋表示中是一样的。在某种意思上说，这种表示方法是一种“倒退”，因为位置索引中能够区分上述两篇文档，不过没关系，我们先由简到难。

词项t的词项频率(以下简称词频)是指t 在d中出现的次数，是与文档相关的一个量，可以认为是文档内代表度的一个量，也可以认为是一种局部信息。检索query的词在文档中出现的频次越高，则相关性也越高，这似乎已经成为我们的一个共识。

但是，词频是可以很大的，值域太大可能造成数值评估不鲁棒。为了平滑一下这个过程，使用一种更平滑的方式：
$$
w=1+\log tf
$$
除词项频率tf之外，我们还想利用词项在整个文档集中的频率进行权重和评分计算。这就又引出另一个分数——IDF。

罕见词项比常见词所蕴含的信息更多。比如，考虑查询中某个词项，它在整个文档集中非常罕见，某篇包含该词项的文档很可能相关。于是，我们希望像ARACHNOCENTRIC一样的罕见词项将有较高权重。物以稀为贵！对于罕见词项我们希望赋予高权重；对于常见词我们希望赋予正的低权重！

不知道大家还记不记得前面提过的一个概念就是文档频率，指的是一个词在多少个文档中出现。所谓逆文档频率其实就是文档频率做了一个反向变换：
$$
idf=\log\frac{N}{df}
$$
那么，所谓TF-IDF，就是把tf和idf乘起来：
$$
TFIDF=(1+\log tf)\log\frac{N}{df}
$$

## 向量空间模型初探

在前面讲布尔检索的时候，我们把每个文档表示为一个布尔向量。现在有了TF-IDF值，我们又可以用每个词的TF-IDF值替换0-1，形成一个TF-IDF向量。于是，我们有一个 |V|维实值空间空间的每一维都对应词项，文档都是该空间下的一个点或者向量。但对于Web搜索引擎来说，由于文档实在太多，空间会上千万维；对每个向量来说又非常稀疏，大部分都是0。

好吧，anyway，我们现在可以用向量化的方法来表示词和文档了。我们也可以把查询内容看成向量。

回想一下，我们是希望和布尔模型不同，能够得到非二值的、既不是过多或也不是过少的检索结果。这里，我们通过计算出相关文档的相关度高于不相关文档相关度的方法来实现。

向量空间下相似度怎么度量呢？

一种比较纯粹的想法是用欧氏距离。但是欧氏距离下就有个问题，那就是语义相近但表达长度不同的文档欧氏距离可以隔得很远，这并不是一个很好的考虑。

### 欧氏距离

**定义与公式**：
欧氏距离是最常用的距离度量方法之一，用于计算两个向量之间的直线距离。对于两个n维向量 $ \vec{X} = (x_1, x_2, \ldots, x_n)$ 和 $\vec{Y} = (y_1, y_2, \ldots, y_n)$，其欧氏距离定义为：
$$
d_E(\vec{X}, \vec{Y}) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + \cdots + (x_n - y_n)^2}
$$
**动机与应用场景**：

- **动机**：欧氏距离简单直观，适用于需要比较两个向量在各维度上的绝对差异的场景。它能够直接反映向量之间的“距离”大小。
- **应用场景**：在文本相似度评估中，欧氏距离常用于比较两个文本向量在各特征维度上的差异。例如，使用词频向量表示文本时，欧氏距离可以衡量两个文本在词频分布上的差异。

**缺点**：
- **维度影响**：欧氏距离对各维度的量纲敏感，不同维度的量纲差异可能导致结果不准确。例如，在文本中，词频和词长的量纲不同，直接使用欧氏距离可能导致偏差。
- **稀疏性问题**：在高维空间中，文本向量通常是稀疏的，欧氏距离在这种情况下可能不够有效。

### 余弦相似度

**定义与公式**：
余弦相似度用于衡量两个向量在方向上的相似程度，而不考虑其长度。对于两个向量 $\vec{A}$ 和 $\vec{B}$，其余弦相似度定义为：
$$
\text{cos}(\theta) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \|\vec{B}\|}
$$

其中，$\vec{A} \cdot \vec{B}$ 是向量的点积，$\|\vec{A}\|$ 和 $\|\vec{B}\|$ 是向量的模。

**动机与应用场景**：
- **动机**：在文本相似度评估中，我们通常更关心文本的语义方向而不是绝对的词频大小。余弦相似度能够有效地解决欧氏距离在高维空间中稀疏性问题，更注重向量的方向。
- **应用场景**：广泛应用于推荐系统、文本分类、信息检索等领域。例如，在推荐系统中，通过计算用户兴趣向量与物品特征向量的余弦相似度，可以推荐与用户兴趣最匹配的物品。

**优点**：
- **不受长度影响**：余弦相似度不受向量长度的影响，更加关注向量的方向，适合用于比较文本的语义相似性。
- **适用于高维稀疏数据**：在处理高维稀疏的文本数据时，余弦相似度能够有效地捕捉向量之间的方向关系。

通过使用欧氏距离和余弦相似度，我们可以从不同的角度评估文本的相似度，选择合适的方法可以更好地满足特定应用场景的需求.

![1735641244580](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735641244580.png)



