# 更多向量空间计算方法

## 更多的TF-IDF

![1735642635433](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735642635433.png)

对于查询和文档常常采用不同的权重计算机制

- 记法: ddd.qqq，前3位代表文档向量的权重的计算方法，后3位代表查询向量的权重计算方法
- 每个3位字母组合中的第1位表示tf因子，第2位df因子，第3位表示归一化形式
- 例如: lnc.ltn
  - 文档: 对数tf，无idf因子，余弦长度归一化
  - 查询: 对数tf，idf，无归一化
- 文档当中不用idf结果会不会很差？
  - 查询: “best car insurance”
  - 文档: “car insurance auto insurance”

扩展布尔模型（Extended Boolean Model, EBM）是信息检索中的一种查询模型，旨在克服传统布尔模型的局限性，同时保留其简单性和直观性。扩展布尔模型通过引入权重和模糊逻辑，使得查询更加灵活和有效。

### 传统布尔模型的局限性
- **过于严格**：传统布尔模型要求文档完全满足查询条件，即文档必须包含所有查询词项。这导致检索结果的覆盖率较低，容易遗漏一些相关但不完全匹配的文档。
- **缺乏排序**：布尔模型不提供对检索结果的排序机制，所有满足条件的文档被视为同等重要，无法根据相关性进行排序。

### 扩展布尔模型的基本思想
- **引入权重**：在扩展布尔模型中，每个查询词项都被赋予一个权重，表示其在查询中的重要性。文档的相关性得分是基于其包含查询词项的程度和这些词项的权重计算得出的。
- **模糊逻辑**：允许文档在一定程度上满足查询条件，而不是完全满足。通过模糊逻辑，可以处理词项的部分匹配和近义词的情况，从而提高检索的灵活性和覆盖率.

### 扩展布尔模型的实现
- **查询表示**：
  - 查询被表示为一个加权词项集合，每个词项都有一个对应的权重。例如，查询“信息检索”可以表示为 $\{(\text{信息}, w_1), (\text{检索}, w_2)\}$，其中 $w_1$和 $w_2$是词项的权重.
- **文档评分**：
  - 对于每个文档，计算其与查询的相关性得分。得分是基于文档中包含的查询词项及其权重计算的。常用的评分函数包括：
    - **加权和**：简单地将文档中包含的查询词项的权重相加，得到文档的相关性得分.
    - **加权平均**：计算文档中包含的查询词项的权重的加权平均值，考虑了文档中词项的数量和权重.
- **排序与检索**：
  - 根据文档的相关性得分对检索结果进行排序，返回得分最高的文档。这样可以将最相关的文档排在前面，提高检索的准确性和用户体验.

### 扩展布尔模型的优点
- **灵活性**：通过引入权重和模糊逻辑，扩展布尔模型能够更好地处理复杂的查询需求，提供更灵活的检索方式.
- **排序能力**：能够对检索结果进行排序，根据文档的相关性得分返回最相关的文档，提高检索的准确性和用户体验.
- **简单易懂**：保留了布尔模型的简单性和直观性，用户可以更容易地理解和使用查询表达式.

### 应用场景
- **文本检索**：适用于各种文本检索系统，如搜索引擎、图书馆目录检索等，能够提供更准确和灵活的检索结果.
- **信息过滤**：在信息过滤系统中，可以用于根据用户的兴趣和需求对信息进行筛选和排序，提供个性化的信息推荐.

扩展布尔模型在信息检索中提供了一种介于严格布尔检索和基于概率的检索之间的方法，通过引入权重和模糊逻辑，克服了传统布尔模型的局限性，提高了检索的灵活性和准确性.

## 词向量技术简介

**词向量（Word Vector）** 是一种将自然语言中的词语转化为计算机可以理解的数值向量的方法。在自然语言处理中，词语本身是离散的符号，无法直接用于数学计算，因此需要通过词向量将其映射到一个连续的向量空间中。词向量的核心思想是，语义相近的词语在向量空间中的位置也相近。例如，假设我们用三维向量表示词语，"猫" 可以表示为 [0.2, 0.8, 0.1]，而 "狗" 可以表示为 [0.3, 0.7, 0.2]，这两个向量在空间中距离较近，因为它们都是宠物；而 "汽车" 可能表示为 [0.9, 0.1, 0.4]，与 "猫" 和 "狗" 的距离较远。通过这种方式，词向量不仅能够表示词语本身，还能捕捉词语之间的语义关系，如相似性、类比关系等。词向量的典型应用包括机器翻译、文本分类和情感分析等。（大家可以看看我们这边NLP和大模型相关的教程，里面讲的会详细一些）

![1735643113055](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735643113055.png)

词向量之间的运算关系能够揭示词语之间的语义和语法规律。例如，通过简单的线性运算，可以捕捉到词语之间的类比关系。经典的例子是“国王 - 男人 + 女人 ≈ 女王”，用词向量表示为：**vec(国王) - vec(男人) + vec(女人)** 的结果会非常接近 **vec(女王)**。这是因为词向量在训练过程中学习到了词语之间的分布特征，使得语义关系可以通过向量的加减来体现。类似地，像“首都”这样的关系也可以通过运算表示，比如“巴黎 - 法国 + 中国 ≈ 北京”，这表明词向量不仅能够表示词语本身的含义，还能捕捉到实体之间的复杂关系。这种特性使得词向量在自然语言处理任务中具有强大的表达能力，广泛应用于机器翻译、问答系统和文本生成等领域。

![1735643099619](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735643099619.png)

说到词向量，那就不得不提到词向量的老祖宗——统计语言模型。一种最经典的语言模型那就是N-Grams模型。**N-Grams语言模型** 是一种基于统计的自然语言处理模型，用于预测文本中下一个词或字符的概率。它的核心思想是通过前 $ N-1 $ 个词（或字符）来预测第 $ N $ 个词（或字符）。N-Grams 中的 “N” 表示模型考虑的上下文长度，例如，当 $ N=2 $ 时，称为二元模型（Bigram），模型会根据前一个词预测当前词；当 $ N=3 $ 时，称为三元模型（Trigram），模型会根据前两个词预测当前词，以此类推。

N-Grams 模型的概率计算基于语料库中的词频统计。例如，在 Bigram 模型中，词 $ w_i $ 在词 $ w_{i-1} $ 之后出现的概率可以表示为：
$$
P(w_i | w_{i-1}) = \frac{\text{count}(w_{i-1}, w_i)}{\text{count}(w_{i-1})}
$$
其中，$\text{count}(w_{i-1}, w_i)$ 表示词对 $ (w_{i-1}, w_i) $ 在语料库中出现的次数，$\text{count}(w_{i-1})$ 表示词 $ w_{i-1} $ 出现的次数。N-Grams 模型简单高效，能够捕捉局部上下文信息，但也存在数据稀疏问题，即当某些词组合在语料库中未出现时，模型无法准确估计其概率。为了解决这一问题，通常会采用平滑技术（如拉普拉斯平滑）来分配少量概率给未出现的词组合。N-Grams 模型广泛应用于文本生成、拼写纠正和语音识别等领域。

![1735643136079](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735643136079.png)

**Word2Vec** 是一种用于生成词向量的神经网络模型，由 Google 在 2013 年提出。它通过将词语映射到低维连续向量空间中，捕捉词语之间的语义和语法关系。Word2Vec 的核心思想是“分布假设”，即具有相似上下文的词语在语义上也相似。Word2Vec 包含两种主要模型：**CBOW（Continuous Bag of Words）** 和 **Skip-Gram**。

1. **CBOW 模型**：  
   CBOW 通过上下文词语预测目标词。例如，给定句子“The cat sits on the mat”，如果目标词是“sits”，则模型会利用上下文词语“The”、“cat”、“on”、“the”、“mat”来预测“sits”。CBOW 适合处理小型数据集，训练速度较快。

2. **Skip-Gram 模型**：  
   Skip-Gram 与 CBOW 相反，它通过目标词预测上下文词语。例如，给定目标词“sits”，模型会预测其周围的词语“The”、“cat”、“on”、“the”、“mat”。Skip-Gram 在处理大型数据集时表现更好，尤其适合捕捉稀有词语的关系。

Word2Vec 使用 softmax 函数计算词语的概率分布。例如，在 Skip-Gram 中，目标词 $ w_t $ 和上下文词 $ w_c $ 的条件概率为：

$$
P(w_c | w_t) = \frac{\exp(\mathbf{v}_{w_t} \cdot \mathbf{v}_{w_c}')}{\sum_{w \in V} \exp(\mathbf{v}_{w_t} \cdot \mathbf{v}_{w}')}
$$
其中，$ \mathbf{v}_{w_t} $ 是目标词的向量，$ \mathbf{v}_{w_c}' $ 是上下文词的向量，$ V $ 是词汇表。

下图展示了 Skip-Gram 模型的结构：

```
输入层 (目标词) → 隐藏层 (词向量) → 输出层 (上下文词概率分布)
```

1. **输入层**：目标词的 one-hot 编码。  
2. **隐藏层**：词向量（低维稠密向量）。  
3. **输出层**：通过 softmax 计算上下文词的概率分布。

Word2Vec 广泛应用于文本分类、机器翻译和推荐系统等领域，是自然语言处理中的重要工具。