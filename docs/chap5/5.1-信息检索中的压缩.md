# 信息检索中的压缩

## 为什么要压缩

我给你们看看谷歌荷兰的数据中心。

![1735632455603](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735632455603.png)

谷歌有着全球最大的商业PC服务器集群，超过250万台服务器，上面存了万亿级别的网页，每天哪怕是搜，都要搜2个亿的网页。你想想，一天就24个小时，这么大网页的搜索量，平均一个网页的响应时间要多短，如果索引不压缩，Google的查询响应时间是多少？

我给大家简单算一笔账。我们如果服务器上有一百个亿的网页，每个网页平均五百个token，平均一个token长度为5个字符，那存下它们要多少啊，30TB。好，假设一块30T的硬盘，传输速度100MB/s，就这么顺序扫过去，扫一趟那得整整4天都未必扫的完，中途一旦打个岔那废了。

怎么更快地查询呢？

如果我们构建倒排索引，好，假设每个文档里面有250个词条是独立的，词项和文档对可以降个数量级到十亿级别。倒排表就是12.5TB，少了不少了。字典更小，只要1.4G就够。假设information出现在一个亿个网页中，Retrieval出现在一百万个网页中，忽略CPU的延迟等因素，一共只要50.5秒。诶，这个速度打下来好多了。但我们还嫌弃它有点慢。

如何让查询更快呢？这就是今天的主题，压缩。压缩的本质就是把长串编码用短串子代替。

- 减少磁盘空间 (节省开销)
- 增加内存缓存内容 (加快速度)
- 加快从磁盘到内存的数据传输速度 (同样加快速度)
  - [读压缩数据到内存+在内存中解压] 比直接读入未压缩数据要快很多
  - 前提: 解压速度很快

为什么需要压缩？

- 首先，需要考虑词典的存储空间
  - 词典压缩的主要动机: 使之能够尽量放入内存中
- 其次，对于倒排记录表而言
  - 动机: 减少磁盘存储空间，减少从磁盘读入内存的时间
- 注意: 大型搜索引擎将相当比例的倒排记录表都放入内存

（简介有损压缩和无损压缩）

## 词典的统计特性

在词典压缩时，词典大小是关键；倒排记录表里面，词项的分布情况是关键。那么，怎么描述这两个事呢？看看下面这个例子：

![1735633386961](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735633386961.png)

- 第2列给出了经过不同程度预处理后的词项数目，该数目是决定词汇表大小的主要因素

- 第3列给出了无位置信息倒排记录的数目，反映了无位置信息索引的预期大小

- 总体来说，预处理对词典大小和无位置信息倒排记录的数目影响很大

  **30定律（rule of 30）：出现频率最高的30个词在书面文本占了30%的出现比例**

如果剔除出现频率最高的150个词，则无位置信息的倒排记录数目会减少25%~30%。尽管通过剔除频率最高的150个停用词能够将倒排记录的个数减少1/4，但索引的压缩比例不能达到1/4。因此，高频词的文档ID间隔小，压缩之后，高频所占用的空间少，只能减少一小部分压缩空间。

### Heaps定律
Heaps定律是一个用于描述文本语料库中词汇量与语料库大小之间关系的经验法则。它指出，随着语料库中词元数（即总词数）的增加，唯一词汇量的增长遵循一个可预测的模式。

**公式**：
Heaps定律是描述自然语言文本中词汇增长规律的经验法则，指出文本中不同词汇的数量（型例数）与文本总长度（词例数）之间存在幂函数关系。其数学表达式为：

$$ V(n) = k \cdot n^{\beta} $$

其中：
- $V(n)$ 表示文本包含n个词时的不同词汇量
- $n$ 为文本总词数
- $ k $ 和 $\beta$ 为经验参数（通常 $ 0 < \beta < 1 $，英语文本中 $\beta \approx 0.5 $）

该定律广泛应用于信息检索、自然语言处理等领域，为词典规模估计和系统设计提供理论依据。

![1735633469096](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735633469096.png)

### 齐夫定律

齐夫定律是由语言学家乔治·金斯利·齐夫提出的，用于描述自然语言中单词出现频率的分布规律。该定律指出，在自然语言的语料库中，一个单词出现的频率与其在频率表中的排名成反比。

**齐夫定律（Zipf's Law）** 是一种描述自然语言中词频分布的统计规律。它指出，在给定的语料库中，一个词的频率与其在频率表中的排名成反比。具体来说，排名第 \( r \) 的词的频率 \( f_r \) 满足：
$$
f_r \propto \frac{1}{r}
$$
即：

$$
f_r = \frac{C}{r}
$$
其中，C是一个常数。齐夫定律揭示了语言中少数高频词占据了大部分使用频率，而大多数词则出现频率较低的现象。

齐夫定律揭示了语言中常见的幂律分布现象，即少数高频词占据了大部分的出现次数，而大量低频词的出现次数较少.

![1735633485381](C:\Users\马世拓\AppData\Roaming\Typora\typora-user-images\1735633485381.png)

