# 逻辑回归（Logistic Regression）

逻辑回归是一种广泛应用于分类任务的统计学习方法，尤其适用于二分类问题（如判断文档是否相关、邮件是否为垃圾邮件）。尽管名称中包含“回归”，但其本质是通过概率建模实现分类决策，是信息检索、自然语言处理等领域的基础模型之一。

---

## 一、基本概念与数学原理
1. **核心思想**  
   逻辑回归通过**Sigmoid函数**将线性回归的输出映射到 $[0,1]$ 区间，表示样本属于某一类别的概率：  
   $$
   P(y=1 | \boldsymbol{x}) = \sigma(\boldsymbol{w}^T \boldsymbol{x} + b) = \frac{1}{1 + e^{-(\boldsymbol{w}^T \boldsymbol{x} + b)}}
   $$  
   其中：
   - $\boldsymbol{x}$ 为特征向量，$\boldsymbol{w}$ 为权重，$b$ 为偏置项。
   - Sigmoid函数 $\sigma(z) = \frac{1}{1+e^{-z}}$ 将线性组合 $z = \boldsymbol{w}^T \boldsymbol{x} + b$ 转换为概率。

2. **决策边界**  
   - 设定阈值（通常为0.5）：若 $P(y=1|\boldsymbol{x}) \geq 0.5$，则预测为类别1；否则为类别0。  
   - 决策边界为线性超平面 $\boldsymbol{w}^T \boldsymbol{x} + b = 0$，在特征空间中划分两类样本。

---

## 二、模型训练与参数估计
1. **损失函数（对数损失）**  
   逻辑回归通过**极大似然估计（MLE）**求解最优参数，定义损失函数为负对数似然：  
   $$
   \mathcal{L}(\boldsymbol{w}, b) = -\sum_{i=1}^n \left[ y_i \log P(y_i=1|\boldsymbol{x}_i) + (1-y_i) \log (1 - P(y_i=1|\boldsymbol{x}_i)) \right]
   $$  
   目标是最小化损失函数，等价于最大化观测数据的似然概率。

2. **优化方法**  
   - **梯度下降**：通过迭代更新参数 $\boldsymbol{w}$ 和 $b$，沿损失函数负梯度方向调整：  
     $$
     \boldsymbol{w} \leftarrow \boldsymbol{w} - \eta \frac{\partial \mathcal{L}}{\partial \boldsymbol{w}}, \quad b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
     $$  
     其中 $\eta$ 为学习率，梯度计算需利用链式法则（见导数公式推导）。  
   - **牛顿法/拟牛顿法**：二阶优化方法，收敛速度更快，但计算复杂度较高。

3. **正则化**  
   为防止过拟合，可引入正则化项：  
   - **L1正则化（Lasso）**：稀疏化权重，公式为 $\lambda \|\boldsymbol{w}\|_1$。  
   - **L2正则化（Ridge）**：约束权重幅度，公式为 $\frac{\lambda}{2} \|\boldsymbol{w}\|_2^2$。

---

## 三、逻辑回归在信息检索中的应用
1. **点击率预测（CTR Estimation）**  
   - 特征：用户历史行为、查询词、文档属性等。  
   - 输出：用户点击广告或搜索结果的概率，用于排序优化。

2. **文档相关性分类**  
   - 输入：查询-文档对的特征（如TF-IDF、BM25得分、词重叠率）。  
   - 输出：文档与查询相关的概率，支持结果排序。

3. **文本分类与过滤**  
   - 任务：垃圾邮件检测、情感分析、主题分类等。  
   - 扩展：结合词袋模型（Bag-of-Words）或TF-IDF特征构建分类器。

---

## 四、逻辑回归的优缺点
| **优势**                          | **局限性**                      |
|----------------------------------|--------------------------------|
| 模型简单，计算效率高               | 仅能学习线性决策边界            |
| 输出为概率，可解释性强             | 对特征工程依赖较高（需人工构造交互项） |
| 支持在线学习与增量更新             | 对异常值和噪声敏感              |
| 可扩展至多分类（Softmax回归）      | 难以处理高维稀疏数据（需结合正则化）  |

---

## 五、扩展与变体
1. **Softmax回归（多分类逻辑回归）**  
   通过推广Sigmoid函数为Softmax函数，支持多类别分类：  
   $$
   P(y=k|\boldsymbol{x}) = \frac{e^{\boldsymbol{w}_k^T \boldsymbol{x} + b_k}}{\sum_{j=1}^K e^{\boldsymbol{w}_j^T \boldsymbol{x} + b_j}}
   $$  
   其中 $K$ 为类别总数。

2. **贝叶斯逻辑回归**  
   引入先验分布（如高斯先验），通过后验概率推断参数，增强模型鲁棒性。

3. **核逻辑回归**  
   结合核技巧（如RBF核）处理非线性可分问题，代价是计算复杂度增加。

---

## 六、总结
逻辑回归凭借其简洁性、可解释性及概率输出特性，成为分类任务的基准模型：  
1. **核心机制**：通过Sigmoid函数建立线性关系与概率的桥梁；  
2. **训练目标**：极大似然估计与梯度下降优化；  
3. **应用场景**：从搜索结果排序到文本分类，覆盖广泛；  
4. **发展方向**：与深度学习结合（如神经网络最后一层的分类器），或改进为结构化逻辑回归处理复杂数据。  

