# 注意力机制与Transformer

### **自注意力机制（Self-Attention）**  
自注意力机制是Transformer模型的核心组件，其核心思想是通过动态计算输入序列中每个位置与其他位置的相关性，捕捉长距离依赖关系。具体过程如下：  
1. **向量生成**：对于输入序列中的每个词向量，生成三个向量：  
   - **查询向量（Query）**：用于“询问”其他位置的关联性。  
   
   - **键向量（Key）**：用于“回答”其他位置的查询。  
   
   - **值向量（Value）**：存储实际语义信息。  
     数学表示为：  
     $$
     Q = XW^Q, \quad K = XW^K, \quad V = XW^V
  $$
   
     其中，$ X $为输入矩阵，$ W^Q, W^K, W^V $为可学习的权重矩阵。  
   
2. **注意力分数计算**：通过查询向量与键向量的点积衡量相关性，并缩放以避免梯度爆炸：  
   $$
   \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
   $$
   
其中，$ d_k $为键向量的维度，缩放因子$ \sqrt{d_k} $用于稳定训练。  
   
3. **Softmax与加权求和**：通过Softmax归一化分数，得到权重矩阵，对值向量加权求和，生成最终的自注意力输出。  

---

### **多头注意力（Multi-Head Attention）**  
为捕捉不同子空间的语义信息，Transformer将自注意力扩展为多头形式：  
1. 将查询、键、值向量投影到$ h $个子空间，分别计算注意力：  
   $$
   \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
   $$
   
2. 拼接所有头的输出并通过线性变换融合：  
   
   $$
   \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
   $$
   
   其中，$ W^O $为输出权重矩阵。多头机制增强了模型对复杂语义关系的建模能力。  

---

### **Transformer整体架构**  
Transformer由编码器和解码器堆叠而成，核心结构如下：  

#### **编码器（Encoder）**  
1. **自注意力层**：计算输入序列的全局依赖关系。  

2. **前馈神经网络（FFN）**：对每个位置独立进行非线性变换：  
   $$
   \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
   $$
   
3. **残差连接与层归一化**：每层输出为$ \text{LayerNorm}(x + \text{Sublayer}(x)) $，缓解梯度消失。  

#### **解码器（Decoder）**  
1. **掩码自注意力层**：防止当前位置关注未来信息（自回归特性）。  
2. **编码-解码注意力层**：以编码器输出为键值，解码器查询为输入，对齐跨模态信息。  
3. **前馈层与归一化**：结构与编码器一致。  

#### **位置编码（Positional Encoding）**  
为保留序列顺序信息，向输入嵌入中添加位置编码：  
$$
PE_{(pos, 2i)} = \sin(pos/10000^{2i/d}), \\
\quad PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d})
$$

或直接学习得到位置嵌入向量。  

---

### **Transformer的优势**  
1. **并行计算**：自注意力机制无需时序计算，显著提升训练效率。  
2. **长距离依赖**：全局注意力机制有效捕捉序列任意位置的关系。  
3. **可扩展性**：通过堆叠层数（如BERT-base为12层）提升模型容量。  

---

**总结**：自注意力机制通过动态权重分配建模上下文依赖，Transformer凭借其并行性与全局感知能力，成为自然语言处理领域的基石模型，并为BERT、GPT等预训练模型奠定基础。